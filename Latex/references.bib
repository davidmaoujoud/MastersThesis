@Book{h2g2,
  author =    {Adams, Douglas},
  title =     {The Hitchhiker's Guide to the Galaxy},
  publisher = {Del Rey (reprint)},
  year =      1995,
  note =      {ISBN-13: 978-0345391803}}

@Book{pratchett06:_good_omens,
  author =    {Pratchett, Terry and Gaiman, Neil},
  title =     {Good Omens:
               \emph{The Nice and Accurate Prophecies of Agnes Nutter, Witch}},
  publisher = {HarperTorch (reprint)},
  year =      2006,
  note =      {ISBN-13: 978-0060853983}}

@Misc{wiki,
  author =       {Wikipedia},
  title =        {Thesis or dissertation},
  howpublished = {URL: \url{http://en.wikipedia.org/wiki/Thesis_or_dissertation},
                  last checked on 2010-01-07}}


@article{mdpcomp,
  author    = {Raffaello D'Andrea},
  title     = {Solving the Bellman Equation, Dynamic Programming and Optimal Control, ETH Zürich},
  year      = {2019},
  url       = {https://ethz.ch/content/dam/ethz/special-interest/mavt/dynamic-systems-n-control/idsc-dam/Lectures/Optimal-Control/Lecture\%20Notes/Lecture5.pdf}
}


@article{rensetal,
  author    = {Gavin Rens and
               Abhaya Nayak and
               Thomas Meyer},
  title     = {Maximizing Expected Impact in an Agent Reputation Network - Technical
               Report},
  journal   = {CoRR},
  volume    = {abs/1805.05230},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.05230},
  archivePrefix = {arXiv},
  eprint    = {1805.05230},
  timestamp = {Mon, 13 Aug 2018 16:47:59 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-05230},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@phdthesis{phdthesispomdp,
  author       = {Jason D. Williams}, 
  title        = {Partially Observable Markov Decision Processes for Spoken Dialogue Management},
  school       = {University of Cambridge},
  year         = 2006,
  url         = {https://pdfs.semanticscholar.org/bb08/29c9a0c71c39d96afaecd070b1e1c047e53c.pdf}
}

@book{Russell:2009:AIM:1671238,
 author = {Russell, Stuart and Norvig, Peter},
 title = {Artificial Intelligence: A Modern Approach},
 year = {2009},
 isbn = {0136042597, 9780136042594},
 edition = {3rd},
 publisher = {Prentice Hall Press},
 address = {Upper Saddle River, NJ, USA},
} 

@ARTICLE{RePEc:inm:oropre:v:21:y:1973:i:5:p:1071-1088,
title = {The Optimal Control of Partially Observable Markov Processes over a Finite Horizon},
author = {Smallwood, Richard D. and Sondik, Edward J.},
year = {1973},
journal = {Operations Research},
volume = {21},
number = {5},
pages = {1071-1088},
abstract = {This paper formulates the optimal control problem for a class of mathematical models in which the system to be controlled is characterized by a finite-state discrete-time Markov process. The states of this internal process are not directly observable by the controller; rather, he has available a set of observable outputs that are only probabilistically related to the internal state of the system. The formulation is illustrated by a simple machine-maintenance example, and other specific application areas are also discussed. The paper demonstrates that, if there are only a finite number of control intervals remaining, then the optimal payoff function is a piecewise-linear, convex function of the current state probabilities of the internal Markov process. In addition, an algorithm for utilizing this property to calculate the optimal control policy and payoff function for any finite horizon is outlined. These results are illustrated by a numerical example for the machine-maintenance problem.},
url = {https://EconPapers.repec.org/RePEc:inm:oropre:v:21:y:1973:i:5:p:1071-1088}
}

@conference{Pineau-2003-8730,
author = {Joelle Pineau and Geoffrey Gordon and Sebastian Thrun},
title = {Point-based value iteration: An anytime algorithm for POMDPs},
booktitle = {Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)},
year = {2003},
month = {August},
pages = {1025 - 1032},
url = {http://www.cs.cmu.edu/~ggordon/jpineau-ggordon-thrun.ijcai03.pdf}
}


@article{uppersurface,
  author    = {Darius Braziunas},
  title     = {POMDP solution methods, Department of Computer Science, University of Toronto},
  year      = {2003},
  url       = {https://www.techfak.uni-bielefeld.de/~skopp/Lehre/STdKI_SS10/POMDP_solution.pdf}
}


@article{oliehoek,
  author    = {Frans A. Oliehoek and Christopher Amato},
  title     = {A Concise Introduction to Decentralized POMDPs},
  year      = {2015},
  url       = {https://www.fransoliehoek.net/docs/OliehoekAmato16book.pdf}
}





@article{hauskrecht,
  author    = {Milos Hauskrecht},
  title     = {Value-Function Approximations for Partially Observable Markov Decision
               Processes},
  journal   = {CoRR},
  volume    = {abs/1106.0234},
  year      = {2011},
  url       = {http://arxiv.org/abs/1106.0234},
  archivePrefix = {arXiv},
  eprint    = {1106.0234},
  timestamp = {Mon, 13 Aug 2018 16:49:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1106-0234},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{KAELBLING199899,
title = "Planning and acting in partially observable stochastic domains",
journal = "Artificial Intelligence",
volume = "101",
number = "1",
pages = "99 - 134",
year = "1998",
issn = "0004-3702",
doi = "https://doi.org/10.1016/S0004-3702(98)00023-X",
url = "http://www.sciencedirect.com/science/article/pii/S000437029800023X",
author = "Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra",
keywords = "Planning, Uncertainty, Partially observable Markov decision processes",
abstract = "In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions."
}



@article{laplacesmoothing,
  author    = {Masato Kikuchi and
               Mitsuo Yoshida and
               Masayuki Okabe and
               Kyoji Umemura},
  title     = {Confidence Interval of Probability Estimator of Laplace Smoothing},
  journal   = {CoRR},
  volume    = {abs/1709.08314},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.08314},
  archivePrefix = {arXiv},
  eprint    = {1709.08314},
  timestamp = {Wed, 18 Sep 2019 09:45:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1709-08314.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}










@inproceedings{crosssum,
author = {Feng, Zhengzhu and Zilberstein, Shlomo},
year = {2005},
month = {01},
pages = {975-980},
title = {Efficient Maximization in Solving POMDPs.},
url = {http://rbr.cs.umass.edu/shlomo/papers/FZaaai05.pdf}
}

@Article{Nitti2017,
author="Nitti, Davide
and Belle, Vaishak
and De Laet, Tinne
and De Raedt, Luc",
title="Planning in hybrid relational MDPs",
journal="Machine Learning",
year="2017",
month="Dec",
day="01",
volume="106",
number="12",
pages="1905--1932",
abstract="We study planning in relational Markov decision processes involving discrete and continuous states and actions, and an unknown number of objects. This combination of hybrid relational domains has so far not received a lot of attention. While both relational and hybrid approaches have been studied separately, planning in such domains is still challenging and often requires restrictive assumptions and approximations. We propose HYPE: a sample-based planner for hybrid relational domains that combines model-based approaches with state abstraction. HYPE samples episodes and uses the previous episodes as well as the model to approximate the Q-function. In addition, abstraction is performed for each sampled episode, this removes the complexity of symbolic approaches for hybrid relational domains. In our empirical evaluations, we show that HYPE is a general and widely applicable planner in domains ranging from strictly discrete to strictly continuous to hybrid ones, handles intricacies such as unknown objects and relational models. Moreover, empirical results showed that abstraction provides significant improvements.",
issn="1573-0565",
doi="10.1007/s10994-017-5669-x",
url="https://doi.org/10.1007/s10994-017-5669-x"
}

@article{Tuyls_Weiss_2012, title={Multiagent Learning: Basics, Challenges, and Prospects}, volume={33}, url={https://www.aaai.org/ojs/index.php/aimagazine/article/view/2426}, DOI={10.1609/aimag.v33i3.2426}, abstractNote={Multiagent systems (MAS) are widely accepted as an important method for solving problems of a distributed nature. A key to the success of MAS is efficient and effective multiagent learning (MAL). The past twenty-five years have seen a great interest and tremendous progress in the field of MAL. This article introduces and overviews this field by presenting its fundamentals, sketching its historical development and describing some key algorithms for MAL. Moreover, main challenges that the field is facing today are indentified.}, number={3}, journal={AI Magazine}, author={Tuyls, Karl and Weiss, Gerhard}, year={2012}, month={Sep.}, pages={41} }

@article{fundamentallymoredifficult,
  author    = {Pablo Hernandez{-}Leal and
               Bilal Kartal and
               Matthew E. Taylor},
  title     = {Is multiagent deep reinforcement learning the answer or the question?
               {A} brief survey},
  journal   = {CoRR},
  volume    = {abs/1810.05587},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.05587},
  archivePrefix = {arXiv},
  eprint    = {1810.05587},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-05587},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{RLgeneral,
  author    = {Sanyam Kapoor},
  title     = {Multi-Agent Reinforcement Learning: {A} Report on Challenges and Approaches},
  journal   = {CoRR},
  volume    = {abs/1807.09427},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.09427},
  archivePrefix = {arXiv},
  eprint    = {1807.09427},
  timestamp = {Mon, 13 Aug 2018 16:48:47 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-09427},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cursedim,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/2171751},
 abstract = {This paper introduces random versions of successive approximations and multigrid algorithms for computing approximate solutions to a class of finite and infinite horizon Markovian decision problems (MDPs). We prove that these algorithms succeed in breaking the "curse of dimensionality" for a subclass of MDPs known as discrete decision processes (DDPs).},
 author = {John Rust},
 journal = {Econometrica},
 number = {3},
 pages = {487--516},
 publisher = {[Wiley, Econometric Society]},
 title = {Using Randomization to Break the Curse of Dimensionality},
 volume = {65},
 year = {1997}
}

@article{nonstation,
author = {Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
year = {2008},
month = {04},
pages = {156 - 172},
title = {A Comprehensive Survey of Multiagent Reinforcement Learning},
volume = {38},
journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
doi = {10.1109/TSMCC.2007.913919}
}

@article{reduce,
  author    = {He He and
               Jordan L. Boyd{-}Graber and
               Kevin Kwok and
               Hal Daum{\'{e}} III},
  title     = {Opponent Modeling in Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1609.05559},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.05559},
  archivePrefix = {arXiv},
  eprint    = {1609.05559},
  timestamp = {Mon, 13 Aug 2018 16:48:46 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeBKD16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reduce2,
author = {Neto, Goncalo},
year = {2005},
month = {01},
pages = {},
title = {From Single-Agent to Multi-Agent Reinforcement Learning: Foundational Concepts and Methods},
url = {http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/learningNeto05.pdf}
}

@article{rensetal,
  author    = {Gavin Rens and
               Abhaya Nayak and
               Thomas Meyer},
  title     = {Maximizing Expected Impact in an Agent Reputation Network - Technical
               Report},
  journal   = {CoRR},
  volume    = {abs/1805.05230},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.05230},
  archivePrefix = {arXiv},
  eprint    = {1805.05230},
  timestamp = {Mon, 13 Aug 2018 16:47:59 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-05230},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cursedim2,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/2171751},
 abstract = {This paper introduces random versions of successive approximations and multigrid algorithms for computing approximate solutions to a class of finite and infinite horizon Markovian decision problems (MDPs). We prove that these algorithms succeed in breaking the "curse of dimensionality" for a subclass of MDPs known as discrete decision processes (DDPs).},
 author = {John Rust},
 journal = {Econometrica},
 number = {3},
 pages = {487--516},
 publisher = {[Wiley, Econometric Society]},
 title = {Using Randomization to Break the Curse of Dimensionality},
 volume = {65},
 year = {1997}
}


@article{convergence,
  author    = {Pieter Abbeel},
  title     = {Learning for Robotics and Control - Value Iteration, CS294-40, University of California, Berkeley},
  year      = {2008},
  url       = {https://inst.eecs.berkeley.edu/~cs294-40/fa08/scribes/lecture2.pdf}
}

@article{vtol2,
  author    = {Tim Hornyak},
  title     = {The flying taxi market may be ready for takeoff, changing the travel experience forever},
  year      = {2020},
  url       = {https://www.cnbc.com/2020/03/06/the-flying-taxi-market-is-ready-to-change-worldwide-travel.html}
}

@article{vtol3,
  author    = {Andrew J. Hawkins},
  title     = {Lilium’s electric air taxi is finally actually flying in new video},
  year      = {2019},
  url       = {https://www.theverge.com/2019/10/22/20925606/lilium-electric-air-taxi-flying-new-test-video}
}


@article{vtol56,
  author    = {Vinod Pawar},
  title     = {Electric Vertical Take-off and Landing (eVTOL) Aircraft Market 2020},
  year      = {2020},
  url       = {https://www.prnewsprime.com/2020/04/electric-vertical-take-off-and-landing-evtol-aircraft-market-2020/}
}



@inproceedings{Boutilier,
 author = {Boutilier, Craig},
 title = {Planning, Learning and Coordination in Multiagent Decision Processes},
 booktitle = {Proceedings of the Sixth Conference on Theoretical Aspects of Rationality and Knowledge},
 year = {1996},
 isbn = {1-55860-417-0},
 pages = {195--210},
 numpages = {16},
 url = {http://dl.acm.org/citation.cfm?id=645875.671730},
 acmid = {671730},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 


@article{decmdp,
author = {Becker, Raphen and Zilberstein, Shlomo and Lesser, Victor and Goldman, Claudia},
year = {2004},
month = {07},
pages = {423-455},
title = {Solving Transition Independent Decentralized Markov Decision Processes},
volume = {22},
journal = {J. Artif. Intell. Res. (JAIR)},
doi = {10.1613/jair.1497}
}

@article{decmdp2,
  author    = {Daniel S. Bernstein and
               Shlomo Zilberstein and
               Neil Immerman},
  title     = {The Complexity of Decentralized Control of Markov Decision Processes},
  journal   = {CoRR},
  volume    = {abs/1301.3836},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3836},
  archivePrefix = {arXiv},
  eprint    = {1301.3836},
  timestamp = {Mon, 13 Aug 2018 16:48:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1301-3836},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{moredec,
author = {Wiering, Marco and Otterlo, Martijn},
year = {2012},
month = {01},
pages = {},
title = {Reinforcement Learning: State-Of-The-Art},
volume = {12},
doi = {10.1007/978-3-642-27645-3}
}

@article{ipomdp,
  author    = {Prashant Doshi and
               Piotr J. Gmytrasiewicz},
  title     = {A Framework for Sequential Planning in Multi-Agent Settings},
  journal   = {CoRR},
  volume    = {abs/1109.2135},
  year      = {2011},
  url       = {http://arxiv.org/abs/1109.2135},
  archivePrefix = {arXiv},
  eprint    = {1109.2135},
  timestamp = {Mon, 13 Aug 2018 16:46:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1109-2135},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{offlineonline,
  author    = {St{\'{e}}phane Ross and
               Joelle Pineau and
               S{\'{e}}bastien Paquet and
               Brahim Chaib{-}draa},
  title     = {Online Planning Algorithms for POMDPs},
  journal   = {CoRR},
  volume    = {abs/1401.3436},
  year      = {2014},
  url       = {http://arxiv.org/abs/1401.3436},
  archivePrefix = {arXiv},
  eprint    = {1401.3436},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RossPPC14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Koenig_2001, title={Agent-Centered Search}, volume={22}, url={https://www.aaai.org/ojs/index.php/aimagazine/article/view/1596}, DOI={10.1609/aimag.v22i4.1596}, abstractNote={In this article, I describe agent-centered search (also called real-time search or local search) and illustrate this planning paradigm with examples. Agent-centered search methods interleave planning and plan execution and restrict planning to the part of the domain around the current state of the agent, for example, the current location of a mobile robot or the current board position of a game. These methods can execute actions in the presence of time constraints and often have a small sum of planning and execution cost, both because they trade off planning and execution cost and because they allow agents to gather information early in nondeterministic domains, which reduces the amount of planning they have to perform for unencountered situations. These advantages become important as more intelligent systems are interfaced with the world and have to operate autonomously in complex environments. Agent-centered search methods have been applied to a variety of domains, including traditional search, strips-type planning, moving-target search, planning with totally and partially observable Markov decision process models, reinforcement learning, constraint satisfaction, and robot navigation. I discuss the design and properties of several agent-centered search methods, focusing on robot exploration and localization.}, number={4}, journal={AI Magazine}, author={Koenig, Sven}, year={2001}, month={Dec.}, pages={109} }

@article{intractt,
  author    = {Trey Smith and
               Reid G. Simmons},
  title     = {Point-Based {POMDP} Algorithms: Improved Analysis and Implementation},
  journal   = {CoRR},
  volume    = {abs/1207.1412},
  year      = {2012},
  url       = {http://arxiv.org/abs/1207.1412},
  archivePrefix = {arXiv},
  eprint    = {1207.1412},
  timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1207-1412},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tipomdp,
author = {Seymour, Richard},
year = {2009},
month = {06},
pages = {101},
title = {The Trust-based Interactive Partially Observable Markov Decision Process}
}

@inproceedings{centralized,
 author = {Lee, Hyun-Rok and Lee, Taesik},
 title = {Improved Cooperative Multi-agent Reinforcement Learning Algorithm Augmented by Mixing Demonstrations from Centralized Policy},
 booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
 series = {AAMAS '19},
 year = {2019},
 isbn = {978-1-4503-6309-9},
 location = {Montreal QC, Canada},
 pages = {1089--1098},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3306127.3331808},
 acmid = {3331808},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 address = {Richland, SC},
 keywords = {cooperative decision making problem, dec-pomdp, imitation learning, multi-agent reinforcement learning},
} 

@Book{Bellman:1957,
  author =       "Bellman, Richard",
  title =        "Dynamic Programming",
  publisher =    "Princeton University Press",
  year =         "1957",
  address =   "Princeton, NJ, USA",
  edition =   "1",
  url = {http://books.google.com/books?id=fyVtp3EMxasC&pg=PR5&dq=dynamic+programming+richard+e+bellman&client=firefox-a#v=onepage&q=dynamic%20programming%20richard%20e%20bellman&f=false},
  bib2html_rescat = "General RL",
}

@article{sondik1973,
 ISSN = {0030364X, 15265463},
 URL = {http://www.jstor.org/stable/168926},
 abstract = {This paper formulates the optimal control problem for a class of mathematical models in which the system to be controlled is characterized by a finite-state discrete-time Markov process. The states of this internal process are not directly observable by the controller; rather, he has available a set of observable outputs that are only probabilistically related to the internal state of the system. The formulation is illustrated by a simple machine-maintenance example, and other specific application areas are also discussed. The paper demonstrates that, if there are only a finite number of control intervals remaining, then the optimal payoff function is a piecewise-linear, convex function of the current state probabilities of the internal Markov process. In addition, an algorithm for utilizing this property to calculate the optimal control policy and payoff function for any finite horizon is outlined. These results are illustrated by a numerical example for the machine-maintenance problem.},
 author = {Richard D. Smallwood and Edward J. Sondik},
 journal = {Operations Research},
 number = {5},
 pages = {1071--1088},
 publisher = {INFORMS},
 title = {The Optimal Control of Partially Observable Markov Processes Over a Finite Horizon},
 volume = {21},
 year = {1973}
}


@book{howard:1960,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Cambridge, MA},
  author = {Howard, R. A.},
  biburl = {https://www.bibsonomy.org/bibtex/28b55f737ee6dd7800ffc7952a33bb6bd/idsia},
  citeulike-article-id = {2380352},
  interhash = {7eed9f4f6bd1f9ee063d80d0f732e48f},
  intrahash = {8b55f737ee6dd7800ffc7952a33bb6bd},
  keywords = {inaki},
  priority = {2},
  publisher = {MIT Press},
  timestamp = {2008-03-11T14:56:12.000+0100},
  title = {Dynamic Programming and Markov Processes},
  year = 1960
}

@unknown{policyadv,
author = {Kristensen, Anders Ringgaard},
year = {1996},
month = {08},
pages = {},
title = {Textbook notes of herd management: Dynamic programming and Markov decision processes},
volume = {49},
journal = {Dina Notat}
}

@ARTICLE{linprog,
title = {A Probabilistic Production and Inventory Problem},
author = {d'Epenoux, F.},
year = {1963},
journal = {Management Science},
volume = {10},
number = {1},
pages = {98-108},
abstract = {R. Howard (R. Howard.1960. Dynamic Programming and Markov Processes. John Wiley and Sons, Inc., New York.) and A. Manne (A. Manne. 1960. Linear programming and sequential decisions. Management Science, April.) have suggested two methods for optimizing average costs per unit time by means of a sequential decision rule. This rule gives order quantities as a function of initial stock levels, and is based upon the assumption of a Markov process which is both homogenous and discrete. The formulation does not include any restrictive hypothesis on the structure of costs. This paper presents a synthesis of both dynamic and linear programming methods applicable to the same kind of model for the case of time discounting.},
url = {https://EconPapers.repec.org/RePEc:inm:ormnsc:v:10:y:1963:i:1:p:98-108}
}

@article{Lovejoy1991ComputationallyFB,
  title={Computationally Feasible Bounds for Partially Observed Markov Decision Processes},
  author={William S. Lovejoy},
  journal={Operations Research},
  year={1991},
  volume={39},
  pages={162-175}
}

@inproceedings{Poon2001AFH,
  title={A fast heuristic algorithm for decision-theoretic planning},
  author={Kin Man Poon},
  year={2001}
}


@article{hsvi,
  author    = {Trey Smith and
               Reid G. Simmons},
  title     = {Heuristic Search Value Iteration for POMDPs},
  journal   = {CoRR},
  volume    = {abs/1207.4166},
  year      = {2012},
  url       = {http://arxiv.org/abs/1207.4166},
  archivePrefix = {arXiv},
  eprint    = {1207.4166},
  timestamp = {Mon, 13 Aug 2018 16:48:32 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1207-4166},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{perseus,
  author    = {Matthijs T. J. Spaan and
               Nikos A. Vlassis},
  title     = {Perseus: Randomized Point-based Value Iteration for POMDPs},
  journal   = {CoRR},
  volume    = {abs/1109.2145},
  year      = {2011},
  url       = {http://arxiv.org/abs/1109.2145},
  archivePrefix = {arXiv},
  eprint    = {1109.2145},
  timestamp = {Mon, 13 Aug 2018 16:46:32 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1109-2145},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{SHOHAM2007365,
title = "If multi-agent learning is the answer, what is the question?",
journal = "Artificial Intelligence",
volume = "171",
number = "7",
pages = "365 - 377",
year = "2007",
note = "Foundations of Multi-Agent Learning",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2006.02.006",
url = "http://www.sciencedirect.com/science/article/pii/S0004370207000495",
author = "Yoav Shoham and Rob Powers and Trond Grenager",
abstract = "The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.11This article has a long history and owes many debts. A first version was presented at the NIPS workshop, Multi-Agent Learning: Theory and Practice, in 2002. A later version was presented at the AAAI Fall Symposium in 2004 [Y. Shoham, R. Powers, T. Grenager, On the agenda(s) of research on multi-agent learning, in: AAAI 2004 Symposium on Artificial Multi-Agent Learning (FS-04-02), AAAI Press, 2004]. Over time it has gradually evolved into the current form, as a result of our own work in the area as well as the feedback of many colleagues. We thank them all collectively, with special thanks to members of the multi-agent group at Stanford in the past three years. Rakesh Vohra and Michael Wellman provided detailed comments on the latest draft which resulted in substantive improvements, although we alone are responsible for the views put forward. This work was supported by NSF ITR grant IIS-0205633 and DARPA grant HR0011-05-1."
}

download as .bib file

@article{DQN,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  archivePrefix = {arXiv},
  eprint    = {1312.5602},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MnihKSGAWR13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{repict,
author = {Vavilis, Sokratis and PetkoviÄ‡, Milan and Zannone, Nicola},
year = {2014},
month = {05},
pages = {},
title = {A Reference Model for Reputation Systems},
volume = {61},
journal = {Decision Support Systems},
doi = {10.1016/j.dss.2014.02.002}
}

@InProceedings{moretrust,
author="Witkowski, Mark
and Artikis, Alexander
and Pitt, Jeremy",
editor="Falcone, Rino
and Singh, Munindar
and Tan, Yao-Hua",
title="Experiments in Building Experiential Trust in a Society of Objective-Trust Based Agents",
booktitle="Trust in Cyber-societies",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="111--132",
abstract="In this paper we develop a notion of ``objective trust'' for Software Agents, that is trust of, or between, Agents based on actual experiences between those Agents. Experiential objective trust allows Agents to make decisions about how to select other Agents when a choice has to be made. We define a mechanism for such an ``objective Trust-Based Agent'' (oTB-Agent), and present experimental results in a simulated trading environment based on an Intelligent Networks (IN) scenario. The trust one Agent places in another is dynamic, updated on the basis of each experience. We use this to investigate three questions related to trust in Multi-Agent Systems (MAS), first how trust affects the formation of trading partnerships, second, whether trust developed over a period can equate to ``loyalty'' and third whether a less than scrupulous Agent can exploit the individual nature of trust to its advantage.",
isbn="978-3-540-45547-9"
}

@article{searchheur,
author = {Hansen, Eric and Zilberstein, Shlomo},
year = {2001},
month = {06},
pages = {35-62},
title = {LAO∗: A heuristic search algorithm that finds solutions with loops},
volume = {129},
journal = {Artificial Intelligence},
doi = {10.1016/S0004-3702(01)00106-0}
}

@article{repp,
author = {Aberer, Karl and Despotovic, Zoran},
year = {2019},
month = {12},
pages = {},
title = {On Reputation in Game Theory Application on Online Settings}
}


@conference {2590,
	title = {Coordinate to cooperate or compete: abstract goals and joint intentions in social interaction},
	booktitle = {Proceedings of the 38th Annual Conference of the Cognitive Science Society},
	year = {2016},
	author = {Max Kleiman-Weiner and Ho, Mark K and Austerweil, Joe L and Michael L, Littman and Joshua B. Tenenbaum}
}


@misc{cassano2019isl,
    title={ISL: Optimal Policy Learning With Optimal Exploration-Exploitation Trade-Off},
    author={Lucas Cassano and Ali H. Sayed},
    year={2019},
    eprint={1909.06293},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{epsi,
author = {Tokic, Michel},
year = {2010},
month = {09},
pages = {203-210},
title = {Adaptive ε-Greedy Exploration in Reinforcement Learning Based on Value Differences},
doi = {10.1007/978-3-642-16111-7_23}
}


@article{vtol,
author = {Bacchini, Alessandro and Cestino, E.},
year = {2019},
month = {02},
pages = {26},
title = {Electric VTOL Configurations Comparison},
volume = {6},
journal = {Aerospace},
doi = {10.3390/aerospace6030026}
}


@inproceedings{bout2,
author = {Boutilier, Craig and Reiter, Raymond and Price, Bob},
year = {2001},
month = {01},
pages = {690-700},
title = {Symbolic Dynamic Programming for First-Order MDPs.}
}


@inproceedings{relationallogic,
author = {Joshi, Saket and Kersting, Kristian and Khardon, Roni},
year = {2010},
month = {01},
pages = {89-96},
title = {Self-Taught Decision Theoretic Planning with First Order Decision Diagrams.}
}



@article{rrmdp,
author = {Wang, Chenggang and Joshi, Saket and Khardon, Roni},
title = {First Order Decision Diagrams for Relational MDPs},
year = {2008},
issue_date = {January 2008},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {31},
number = {1},
issn = {1076-9757},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {431–472},
numpages = {42}
}
  


@book{masbook,
author = {Weiss, Gerhard},
title = {Multiagent Systems},
year = {2013},
isbn = {0262018896},
publisher = {The MIT Press}
}



@article{articlepomh,
author = {Paquet, Sebastien and Chaib-draa, Brahim and Ross, Stephane},
year = {2006},
month = {01},
pages = {},
title = {Hybrid POMDP algorithms}
}




@inproceedings{maniloff,
author = {Maniloff, Diego and Gmytrasiewicz, Piotr},
year = {2011},
month = {01},
pages = {},
title = {Hybrid Value Iteration for POMDPs.}
}


@inproceedings{rtbss,
author = {Paquet, SĂ©bastien and Tobin, Ludovic and Chaib-draa, Brahim},
year = {2005},
month = {01},
pages = {970-977},
title = {An online POMDP algorithm for complex multiagent environments},
doi = {10.1145/1082473.1082620}
}


@inproceedings{rtdpbel,
  title={Solving Large POMDPs using Real Time Dynamic Programming},
  author={Hector Geffner and Blai Bonet},
  year={1998}
}


@MISC{qmdp,
    author = {Michael L. Littman and Anthony R. Cassandra and Leslie Pack Kaelbling},
    title = {Learning policies for partially observable environments: Scaling up},
    year = {1995}
}


@article{monteipomdp,
author = {Doshi, Prashant and Gmytrasiewicz, Piotr},
year = {2009},
month = {01},
pages = {297-337},
title = {Monte Carlo Sampling Methods for Approximating Interactive POMDPs},
volume = {34},
journal = {J. Artif. Intell. Res. (JAIR)},
doi = {10.1613/jair.2630}
}



@article{impomdprel,
author = {Panella, A. and Gmytrasiewicz, Piotr},
year = {2011},
month = {01},
pages = {52-59},
title = {Interactive first-order probabilistic logic},
journal = {AAAI Workshop - Technical Report}
}



@article{impomdpnn,
author = {Han, Yanlin and Gmytrasiewicz, Piotr},
year = {2019},
month = {07},
pages = {6062-6069},
title = {IPOMDP-Net: A Deep Neural Network for Partially Observable Multi-Agent Planning Using Interactive POMDPs},
volume = {33},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v33i01.33016062}
}